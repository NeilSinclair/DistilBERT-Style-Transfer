{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT for Style Transfer - Yelp-v4.ipynb","provenance":[{"file_id":"16X6u9Jdejc7fI45fea64uWVXM--qd2-8","timestamp":1610220381790},{"file_id":"1Qt9ApAqRyjA4b4sBTWHbyoK2-ttJJyxC","timestamp":1610193123129},{"file_id":"1dK0Itrt6AtpNhz2tpSXWSlJ6j2zLQ0b6","timestamp":1610106192638},{"file_id":"18GpbwKBAgxXwQ73lTVZAy1M78ehvAFSL","timestamp":1610046675348},{"file_id":"1fHFuqA2nxwku8ytGWVVQiArxrRfoRW67","timestamp":1608383431396},{"file_id":"1g1M3jWVBDQGUp2RtH7cF3hnW48xalP_0","timestamp":1605515067216},{"file_id":"18a_Hsz2llHauu2zbUVvxOXncFWcm8qW5","timestamp":1605016612545},{"file_id":"1PE67Fsh3Iz5Bm4gK9YrXcMJWu5ukrYb5","timestamp":1600539357919}],"collapsed_sections":["jaRa7YwhUgrY","jXLzSgTOMDwV"],"authorship_tag":"ABX9TyMkoNZ2/J/2d+A9B4geEvua"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"B7EVTfMVdv4Z","executionInfo":{"status":"ok","timestamp":1610300719389,"user_tz":-120,"elapsed":3781,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["# !pip install -q pytorch-lightning\n","!pip install -q transformers"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Goj88fbbQ3Wo","executionInfo":{"status":"ok","timestamp":1610300719392,"user_tz":-120,"elapsed":3498,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"57d486d5-3f1f-4a26-fc09-edc96f2e42b9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'Integrated Gradients/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"stz9MKJIEM7d","executionInfo":{"status":"ok","timestamp":1610300719393,"user_tz":-120,"elapsed":2536,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["import os\r\n","# os.chdir('/content/gdrive/My Drive/Integrated Gradients/')\r\n","# !git clone https://github.com/NeilSinclair/DistilBERT-Style-Transfer.git\r\n","os.chdir('/content/gdrive/My Drive/Integrated Gradients/DistilBERT_Style_Transfer')\r\n","# !git pull"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fDnjiDoeZbK","executionInfo":{"status":"ok","timestamp":1610300722828,"user_tz":-120,"elapsed":5656,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["# imports\n","import transformers\n","from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","import torch.nn.functional as F\n","import torch.nn\n","# import pytorch_lightning as pl\n","import torch\n","# from pytorch_lightning.callbacks import ModelCheckpoint\n","import argparse\n","import os\n","\n","from datetime import timedelta\n","import datetime\n","import time\n","\n","from model.utils import *\n","from model.model import *"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1n35cOggZV_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610300730739,"user_tz":-120,"elapsed":12845,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"8f33ce20-0435-4903-9e16-e8d27cc98bba"},"source":["# Load the DistilBERT models\r\n","from transformers import AdamW, DistilBertTokenizer, DistilBertConfig, DistilBertForMaskedLM, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\r\n","\r\n","dbert_model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\r\n","dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n","\r\n","# class_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\r\n","#                                                                   num_labels = 2,\r\n","#                                                                   output_attentions = False,\r\n","#                                                                   output_hidden_states = False)\r\n","\r\n","# class_model = torch.load(base_dir + '/checkpoint_files/yelp_classifier_1epoch_DBert.pth', map_location=torch.device('cpu'))\r\n","if torch.cuda.is_available():\r\n","  class_model = torch.load(base_dir + '/checkpoint_files/yelp_classifier_1epoch_DBert.pth')\r\n","else: \r\n","  class_model = torch.load(base_dir + '/checkpoint_files/yelp_classifier_1epoch_DBert.pth', map_location=torch.device('cpu'))\r\n","class_model.eval()\r\n","\r\n","## Add special tokens to the models for <pos> and <neg>\r\n","special_tokens_dict = {'additional_special_tokens' : ['<pos>', '<neg>']}\r\n","\r\n","num_added_toks = dbert_tokenizer.add_special_tokens(special_tokens_dict)\r\n","print('We have added', num_added_toks, 'tokens')\r\n","# Resize the token embeddings\r\n","dbert_model.resize_token_embeddings(len(dbert_tokenizer))\r\n","class_model.resize_token_embeddings(len(dbert_tokenizer))\r\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["We have added 2 tokens\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Embedding(30524, 768)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"OQL8LVGQ9x7l","executionInfo":{"status":"ok","timestamp":1610300735229,"user_tz":-120,"elapsed":1431,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["hparams = argparse.Namespace()\r\n","\r\n","hparams.lm_freeze_encoder = False\r\n","# Train the embeddings because we have <pos> and <neg> style tokens now\r\n","hparams.lm_freeze_pos_embeds = True\r\n","hparams.lm_freeze_token_embeds = False\r\n","\r\n","hparams.classifier_freeze_whole_model = True\r\n","# Train the embeddings because we have <pos> and <neg> style tokens now\r\n","hparams.classifier_freeze_pos_embeds = True\r\n","hparams.classifier_freeze_token_embeds = True\r\n","\r\n","hparams.gs_hard = True\r\n","hparams.gs_tau = 1\r\n","\r\n","hparams.eval_beams = 4\r\n","hparams.max_gen_length = 32"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uz-GGmWmICAa","executionInfo":{"status":"ok","timestamp":1610300742178,"user_tz":-120,"elapsed":1353,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["class CombinedModel(torch.nn.Module):\r\n","  def __init__(self, lm_model, class_model, tokenizer, hparams, device = 'cuda'):\r\n","    super(CombinedModel, self).__init__()\r\n","    self.lm_model = lm_model\r\n","    self.class_model = class_model\r\n","    self.device = device\r\n","    self.tokenizer = tokenizer\r\n","\r\n","    self.vocab = torch.FloatTensor(np.arange(0, len(tokenizer))).to(self.device)\r\n","    self.gs_tau = hparams.gs_tau\r\n","    self.gs_hard = hparams.gs_hard\r\n","\r\n","    # Set the MLM model to train and the classification model to evaluation\r\n","    self.lm_model.train()\r\n","    self.class_model.train()\r\n","    self.lm_model.to(self.device)\r\n","    self.class_model.to(self.device)\r\n","\r\n","    self.criterion = torch.nn.CrossEntropyLoss()\r\n","\r\n","    # This selects which parts of the two models are frozen for training\r\n","    model_methods = [lm_model.distilbert, # Note here that we don't freeze the language model layer\r\n","                 lm_model.distilbert.embeddings.word_embeddings,\r\n","                 lm_model.distilbert.embeddings.position_embeddings,\r\n","                 class_model.distilbert, # Freeze the whole of the model, including classifier head\r\n","                 class_model.distilbert.embeddings.word_embeddings,\r\n","                 class_model.distilbert.embeddings.position_embeddings]\r\n","    \r\n","    model_params = [hparams.lm_freeze_encoder,\r\n","                    hparams.lm_freeze_pos_embeds,\r\n","                    hparams.lm_freeze_token_embeds,\r\n","                    hparams.classifier_freeze_whole_model,\r\n","                    hparams.classifier_freeze_pos_embeds,\r\n","                    hparams.classifier_freeze_token_embeds]\r\n","\r\n","    ## Freeze / unfreeze parameters depending on what was passed\r\n","    freeze_multiple_params(model_methods, model_params)\r\n","\r\n","  def forward(self, batch, training = True, translate = False):\r\n","    ''' Forward function combining the two neural networks\r\n","    Args: batch - a batch containing multiple items for the model (see below)\r\n","          training - bool indicating whether or not we're in training mode\r\n","          translate - bool indicating whether to swap the first token\r\n","    '''\r\n","    if not training:\r\n","      self.lm_model.eval()\r\n","\r\n","    src_ids = batch['input_ids'].to(self.device)\r\n","    src_mask = batch['attention_mask'].to(self.device)\r\n","    # tgt_ids = batch['labels'].to(device)\r\n","    mask_ids = batch[\"masked_ids\"].to(self.device)\r\n","    class_labels = batch['class_labels'].to(self.device)\r\n","\r\n","    # if in translate mode, swap out the first token; this assumes that the first token is\r\n","    # the style token and that there are only 2 styles\r\n","    if translate:\r\n","      src_ids = self.swap_style_tokens(src_ids)\r\n","\r\n","    outputs = self.lm_model(input_ids = src_ids, attention_mask = src_mask)\r\n","    lm_logits = outputs.logits\r\n","    \r\n","    # Just get the logits of the masked tokens\r\n","    new_logits = lm_logits[mask_ids.nonzero()[:,0], mask_ids.nonzero()[:,1], : ].to(self.device)\r\n","\r\n","    # if training, use gumbel softmax to select the token\r\n","    if training:\r\n","      new_tokens_ohe = F.gumbel_softmax(new_logits, tau = self.gs_tau, hard = self.gs_hard)\r\n","    else:\r\n","      # new_tokens = new_logits.argmax(-1).squeeze()\r\n","      new_tokens_ohe = F.gumbel_softmax(new_logits, tau = self.gs_tau, hard = True)\r\n","\r\n","    # Then, add this token back into the original sentence\r\n","    new_sentence_batch = sentence_rewriter(mask_ids, new_tokens_ohe, \r\n","                                           src_ids, self.vocab, \r\n","                                           device = self.device).to(self.device)\r\n","    # Pass this updated sentence through the classifier\r\n","    class_outs = self.class_model(input_ids = new_sentence_batch,\r\n","                              attention_mask = src_mask,\r\n","                              labels = class_labels)\r\n","\r\n","    ce_error = self.criterion(class_outs.logits, class_labels)\r\n","\r\n","    # If we're training, just return the loss, but if not, return the loss,\r\n","    # predictions and true labels so that we can get the accuracy of the model\r\n","    if training:\r\n","      model_outs = {\"loss\" : ce_error}\r\n","    else:\r\n","      preds = class_outs.logits.cpu().detach().numpy().argmax(-1)\r\n","      model_outs = {\"loss\" : ce_error,\r\n","                    \"preds\" : preds,\r\n","                    \"true_labels\" : class_labels}\r\n","  \r\n","    return model_outs\r\n","\r\n","  def swap_style_tokens(self, batch):\r\n","    ''' Function that swaps the first token in a sentence with it's opposite\r\n","    Args: batch - a batch of tokens of size [B, sentence_length]\r\n","    Returns: the batch with the first token swapped out\r\n","    '''\r\n","    max_token = len(self.tokenizer) - 1\r\n","\r\n","    style2_tensor = torch.ones([batch.size()[0]], dtype = torch.int64, device = self.device) * max_token\r\n","    style1_tensor = torch.ones([batch.size()[0]], dtype = torch.int64, device = self.device) * (max_token-1)\r\n","\r\n","    # Swap the first tokens around; this is similar to the way one can swap 1 and 0 by doing\r\n","    # new_val = (1 - old_value)\r\n","    batch[:, 0] = (style2_tensor - batch[:, 0]) + style1_tensor\r\n","\r\n","    return batch\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hE3SgGr8im0","executionInfo":{"status":"ok","timestamp":1610300745348,"user_tz":-120,"elapsed":1810,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["combined_model = CombinedModel(dbert_model, class_model, tokenizer = dbert_tokenizer, hparams = hparams, device = 'cuda')\r\n","# combined_model = torch.load(base_dir + '/checkpoint_files/combined_distilBERT_2epoch.pth', map_location=torch.device('cpu'))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOxngTGw-Ucp","executionInfo":{"status":"ok","timestamp":1610307229213,"user_tz":-120,"elapsed":2469,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["def train(data, combined_model, tokenizer, epochs = 1, device = \"cuda\", batch_report_ = 250, lr = 2e-5):\r\n","  vocab = torch.FloatTensor(np.arange(0, len(tokenizer))).to(device)\r\n","  epochs = epochs\r\n","  t0 = time.time()\r\n","\r\n","  total_train_loss = 0\r\n","  batch_train_loss = 0\r\n","\r\n","  batch_loss = []\r\n","  \r\n","  if device == \"cuda\":\r\n","    batch_report = batch_report_\r\n","  else:\r\n","    batch_report = 1\r\n","\r\n","  # Set up the optimizer for the lm model\r\n","  optimizer = AdamW(combined_model.parameters(),\r\n","                  lr = lr,\r\n","                  eps = 1e-8)\r\n","  print(\" === Starting Training === \")\r\n","  for n in range(epochs):\r\n","    print(f\"Starting epoch {n+1} of {epochs}\")\r\n","    for i, batch in enumerate(data):\r\n","\r\n","      if i % batch_report == 0 and not i == 0:\r\n","        elapsed = format_time(time.time() - t0)\r\n","        avg_batch_loss = batch_train_loss/batch_report\r\n","        batch_loss.append(avg_batch_loss) # Rather get the average batch loss every batch_report batches than every batch\r\n","        print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. --- Avg batch loss {:.4f} over {} batches'.format(i, len(data), elapsed, avg_batch_loss, batch_report))\r\n","        batch_train_loss = 0\r\n","\r\n","      # Run the model and get the logits\r\n","      combined_model.zero_grad()\r\n","\r\n","      # Run the combined model\r\n","      outputs = combined_model(batch)\r\n","      # Get the classification error\r\n","      ce_error = outputs['loss']\r\n","\r\n","      batch_train_loss += ce_error.item() \r\n","\r\n","      ce_error.backward()\r\n","      torch.nn.utils.clip_grad_norm(combined_model.parameters(), 5.0)\r\n","      # Update the parameters using the gradients\r\n","      optimizer.step()\r\n","      # Update the learning rate\r\n","\r\n","      total_train_loss += ce_error\r\n","    return {\"avg_train_loss\" : total_train_loss / len(data),\r\n","            \"batch_loss\" : batch_loss}\r\n","\r\n","\r\n"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PbzhCghz_ir","executionInfo":{"status":"ok","timestamp":1610300795407,"user_tz":-120,"elapsed":1388,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["def validate(data, combined_model, tokenizer, num_batches = 100, device = \"cuda\", translate = False):\r\n","  vocab = torch.FloatTensor(np.arange(0, len(tokenizer))).to(device)\r\n","\r\n","  t0 = time.time()\r\n","\r\n","  total_train_loss = 0\r\n","  batch_train_loss = 0\r\n","\r\n","  batch_loss = []\r\n","  if device == \"cuda\":\r\n","    batch_report = num_batches / 4\r\n","  else:\r\n","    batch_report = 1\r\n","\r\n","  print(\" === Running Validation ===\")\r\n","  true_labels = [] \r\n","  preds = []\r\n","  for i, batch in enumerate(data):\r\n","    src_ids = batch['input_ids'].to(device)\r\n","    src_mask = batch['attention_mask'].to(device)\r\n","    mask_ids = batch[\"masked_ids\"].to(device)\r\n","    class_labels = batch['class_labels'].to(device)\r\n","\r\n","    \r\n","    # Swap the labels if we're translating\r\n","    if translate:\r\n","      true_labels.extend(1 - class_labels.cpu().numpy())\r\n","    else: \r\n","      true_labels.extend(class_labels.cpu().numpy())\r\n","\r\n","    if i % batch_report == 0 and not i == 0:\r\n","      elapsed = format_time(time.time() - t0)\r\n","      print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i, num_batches, elapsed))\r\n","      batch_train_loss = 0\r\n","\r\n","    # Run the combined model\r\n","    outputs = combined_model(batch, training = False, translate = translate)\r\n","    # Get the classification error\r\n","    ce_error = outputs['loss']\r\n","\r\n","    batch_train_loss += ce_error.item()\r\n","    batch_loss.append(ce_error.item())\r\n","\r\n","    total_train_loss += ce_error\r\n","    # Get the classification error (and hope that it back propagates)\r\n","    # Get the predictions\r\n","    preds.append(outputs['preds'])\r\n","    # true_labels.append(outputs['true_labels'].cpu().numpy())\r\n","    \r\n","    if i > num_batches:\r\n","      batch_size = len(src_ids)\r\n","      # Get the accuracy of the predictions\r\n","      val_predictions = []\r\n","      val_true = []\r\n","      for j, it in enumerate(preds):\r\n","        val_predictions.extend(it)\r\n","        # val_true.extend(true_labels)\r\n","      val_acc = np.mean(np.array(val_predictions) == np.array(true_labels))\r\n","      print(\"Validation Accuracy {:.2f}%\".format(val_acc*100))\r\n","      break\r\n","    \r\n","  return {\"avg_train_loss\" : total_train_loss / len(data),\r\n","          \"batch_loss\" : batch_loss,\r\n","          \"val_predictions\" : val_predictions,\r\n","          \"true_labels\" : true_labels}"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"htJhtaDVCtvr","executionInfo":{"status":"ok","timestamp":1610300798148,"user_tz":-120,"elapsed":2014,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["train_file = '/content/gdrive/My Drive/Integrated Gradients/Yelp Data - Masked/Yelp_train_20_DBert.csv'\r\n","dataset = TokenizedDataset(train_file, transform = CreateTokens(dbert_tokenizer, None))                        \r\n","train_data = DataLoader(dataset, collate_fn = dataset.collate_fn, sampler = RandomSampler(dataset), batch_size = 64)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"moCpezzL1Mhb","executionInfo":{"status":"ok","timestamp":1610300802025,"user_tz":-120,"elapsed":1299,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}}},"source":["val_file = '/content/gdrive/My Drive/Integrated Gradients/Yelp Data - Masked/Yelp_dev_20_DBert.csv'\r\n","dataset = TokenizedDataset(val_file, transform = CreateTokens(dbert_tokenizer, None))                        \r\n","val_data = DataLoader(dataset, collate_fn = dataset.collate_fn, sampler = RandomSampler(dataset), batch_size = 32)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSyVSHX7SHcQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610310076178,"user_tz":-120,"elapsed":2836631,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"23a938ad-508a-4b6d-fbaf-48ee86d40db5"},"source":["history = train(train_data, combined_model, dbert_tokenizer, device = 'cuda', epochs = 1)"],"execution_count":35,"outputs":[{"output_type":"stream","text":[" === Starting Training === \n","Starting epoch 1 of 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"],"name":"stderr"},{"output_type":"stream","text":["Batch   250  of  6,707.    Elapsed: 0:01:46. --- Avg batch loss 0.3013 over 250 batches\n","Batch   500  of  6,707.    Elapsed: 0:03:31. --- Avg batch loss 0.3105 over 250 batches\n","Batch   750  of  6,707.    Elapsed: 0:05:17. --- Avg batch loss 0.3002 over 250 batches\n","Batch 1,000  of  6,707.    Elapsed: 0:07:03. --- Avg batch loss 0.2992 over 250 batches\n","Batch 1,250  of  6,707.    Elapsed: 0:08:48. --- Avg batch loss 0.3011 over 250 batches\n","Batch 1,500  of  6,707.    Elapsed: 0:10:34. --- Avg batch loss 0.2977 over 250 batches\n","Batch 1,750  of  6,707.    Elapsed: 0:12:21. --- Avg batch loss 0.2988 over 250 batches\n","Batch 2,000  of  6,707.    Elapsed: 0:14:07. --- Avg batch loss 0.2941 over 250 batches\n","Batch 2,250  of  6,707.    Elapsed: 0:15:53. --- Avg batch loss 0.3024 over 250 batches\n","Batch 2,500  of  6,707.    Elapsed: 0:17:39. --- Avg batch loss 0.2954 over 250 batches\n","Batch 2,750  of  6,707.    Elapsed: 0:19:25. --- Avg batch loss 0.2986 over 250 batches\n","Batch 3,000  of  6,707.    Elapsed: 0:21:11. --- Avg batch loss 0.3003 over 250 batches\n","Batch 3,250  of  6,707.    Elapsed: 0:22:56. --- Avg batch loss 0.2934 over 250 batches\n","Batch 3,500  of  6,707.    Elapsed: 0:24:42. --- Avg batch loss 0.2882 over 250 batches\n","Batch 3,750  of  6,707.    Elapsed: 0:26:27. --- Avg batch loss 0.2820 over 250 batches\n","Batch 4,000  of  6,707.    Elapsed: 0:28:14. --- Avg batch loss 0.2977 over 250 batches\n","Batch 4,250  of  6,707.    Elapsed: 0:29:59. --- Avg batch loss 0.2863 over 250 batches\n","Batch 4,500  of  6,707.    Elapsed: 0:31:45. --- Avg batch loss 0.2909 over 250 batches\n","Batch 4,750  of  6,707.    Elapsed: 0:33:30. --- Avg batch loss 0.2806 over 250 batches\n","Batch 5,000  of  6,707.    Elapsed: 0:35:15. --- Avg batch loss 0.2921 over 250 batches\n","Batch 5,250  of  6,707.    Elapsed: 0:37:01. --- Avg batch loss 0.2909 over 250 batches\n","Batch 5,500  of  6,707.    Elapsed: 0:38:46. --- Avg batch loss 0.2912 over 250 batches\n","Batch 5,750  of  6,707.    Elapsed: 0:40:32. --- Avg batch loss 0.2813 over 250 batches\n","Batch 6,000  of  6,707.    Elapsed: 0:42:18. --- Avg batch loss 0.2841 over 250 batches\n","Batch 6,250  of  6,707.    Elapsed: 0:44:03. --- Avg batch loss 0.2795 over 250 batches\n","Batch 6,500  of  6,707.    Elapsed: 0:45:48. --- Avg batch loss 0.2862 over 250 batches\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NHQi8HZn-Qts"},"source":["torch.save(combined_model, base_dir + '/checkpoint_files/combined_distilBERT_3epoch.pth')\r\n","# torch.save(combined_model, base_dir + '/checkpoint_files/combined_distilBERT_2epoch.pth')\r\n","# combined_model = torch.load(base_dir + '/checkpoint_files/combined_distilBERT_1epoch.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYcTL4eb3TR9","executionInfo":{"status":"ok","timestamp":1610307206931,"user_tz":-120,"elapsed":116677,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"6a99b672-173c-46f3-94ab-80feca3547dd"},"source":["history_val = validate(val_data, combined_model, dbert_tokenizer, num_batches = 500, device = 'cuda', translate = True)"],"execution_count":33,"outputs":[{"output_type":"stream","text":[" === Running Validation ===\n","Batch   125  of    500.    Elapsed: 0:00:27.\n","Batch   250  of    500.    Elapsed: 0:00:53.\n","Batch   375  of    500.    Elapsed: 0:01:20.\n","Batch   500  of    500.    Elapsed: 0:01:46.\n","Validation Accuracy 54.28%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPBwiVQpzi8b","executionInfo":{"status":"ok","timestamp":1610306827627,"user_tz":-120,"elapsed":2083,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"c363027f-3bf7-43a9-dedf-4817daa11905"},"source":["import matplotlib.pyplot as plt\r\n","\r\n","# plt.plot(history['batch_loss'])\r\n","print(history_val['avg_train_loss'] * len(val_data) / 500)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["tensor(0.3123, device='cuda:0', grad_fn=<DivBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iSzqPD8xUpA6"},"source":["def swap_style_tokens(tokenizer, batch):\r\n","  ''' Function that swaps the first token in a sentence with it's opposite\r\n","  Args: tokenizer - the tokenizer object\r\n","        batch - a batch of tokens of size [B, sentence_length]\r\n","  Returns: the batch with the first token swapped out\r\n","  '''\r\n","  max_token = len(tokenizer)\r\n","\r\n","  style2_tensor = torch.ones([batch.size()[0]], dtype = torch.int64) * max_token\r\n","  style1_tensor = torch.ones([batch.size()[0]], dtype = torch.int64) * (max_token-1)\r\n","\r\n","  # Swap the first tokens around; this is similar to the way one can swap 1 and 0 by doing\r\n","  # new_val = (1 - old_value)\r\n","  batch[:, 0] = (style2_tensor - batch[: 0]) + style1_tensor\r\n","\r\n","  return batch\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaRa7YwhUgrY"},"source":["# Storing some random stuff down here - delete later"]},{"cell_type":"code","metadata":{"id":"gHQ6K1g0mB_R"},"source":["test_text = dbert_tokenizer(\"<neg> I was [MASK] about the pizza but then I [MASK] it anyway\",\r\n","                      return_tensors = 'pt', max_length = 16, padding = 'max_length').to('cuda')\r\n","dbert_model.eval()\r\n","tt_ = dbert_model(input_ids = test_text['input_ids'], attention_mask = test_text['attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJTg5FcIvWlO","executionInfo":{"status":"ok","timestamp":1610189086619,"user_tz":-120,"elapsed":1362,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"92d11f9f-4e4e-48c5-c8ff-f856b1cc77f7"},"source":["print([dbert_tokenizer.decode(o) for o in tt_.logits[:, [4, 11], :].argmax(-1)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['thinking ate']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D-6prbU31IEe"},"source":["# dbert_model.to('cpu')\r\n","# a_ = torch.FloatTensor([0,1,0], device = 'cpu').unsqueeze(0) @ dbert_model.distilbert.embeddings.word_embeddings(torch.LongTensor([564, 571, 1002], device = 'cpu')) \r\n","# a_.size()\r\n","\r\n","embed_matrix = dbert_model.distilbert.embeddings.word_embeddings(\r\n","    torch.LongTensor(np.arange(0, len(dbert_tokenizer)))\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pER2KWzD64Lh","executionInfo":{"status":"ok","timestamp":1610192197091,"user_tz":-120,"elapsed":1112,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"a13bc9e4-79ec-49d7-86ee-69eb2d831064"},"source":["import sys\r\n","embed_matrix.size()\r\n","(embed_matrix.element_size() * embed_matrix.nelement()) * 1e-6"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["93.769728"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fr9AuG9VIzkV","executionInfo":{"status":"ok","timestamp":1610273902898,"user_tz":-120,"elapsed":1330,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"fa28b09d-0397-4a0c-86d0-72abb09a2266"},"source":["for d in dbert_model.vocab_transform.parameters():\r\n","  print(d.requires_grad)\r\n","\r\n","for d in dbert_model.vocab_layer_norm.parameters():\r\n","  print(d.requires_grad)\r\n","  \r\n","# dbert_model.vocab_layer_norm.requires_grad"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","True\n","True\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jXLzSgTOMDwV"},"source":["# Validation classification accuracy\n","This section first creates a batch of noised sentences reconstructed conditioned on a stlye token by the trained BART model and then uses the originally trained BERT classifier to test the accuracy of the (re)generated validation sentences"]},{"cell_type":"code","metadata":{"id":"srX1igyD8-Bp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610020136281,"user_tz":-120,"elapsed":1706,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"1f61a199-b823-44a2-b280-479a04c77196"},"source":["# Generate a Dataset object of the validation sentences for a generation pass through BART\n","do_translation = True\n","GEN_BATCH_SIZE = 32\n","# val_data = pd.read_csv(base_dir + 'Yelp Data - Masked/Val_Yelp_4k_80percent.csv').iloc[:1000,:]\n","val_data = pd.read_csv(base_dir + 'Yelp Data - Masked/Yelp_dev.csv')\n","# shuffle and take 1000 examples\n","val_data = val_data.sample(frac = 1)[:1000]\n","val_data_x = val_data['Masked']\n","val_data_y = val_data['Label']\n","\n","val_gen_ids = []\n","val_gen_masks = []\n","\n","for sent in val_data_x:\n","  # Replace these unecessary tokens\n","  sent = re.sub(r'<s> |</s> ', '', sent)\n","  if do_translation:\n","    if '<pos>' in sent:\n","      sent = re.sub(r'<pos>', '<neg>', sent)\n","    else:\n","      sent = re.sub(r'<neg>', '<pos>', sent)\n","\n","  val_enc = tokenizer(sent, max_length=32, truncation = True,\n","                      pad_to_max_length = True, return_tensors = 'pt',\n","                      return_attention_mask = True, \n","                      add_special_tokens = True)\n","  val_gen_ids.append(val_enc['input_ids'])\n","  val_gen_masks.append(val_enc['attention_mask'])\n","\n","val_gen_ids = torch.cat(val_gen_ids, dim = 0)\n","val_gen_masks = torch.cat(val_gen_masks, dim = 0)\n","\n","val_gen_dataset = TensorDataset(val_gen_ids, val_gen_masks, torch.tensor(np.array(val_data_y)))\n","val_gen_dataset = DataLoader(val_gen_dataset, batch_size = GEN_BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4EfxBeyMdYq9"},"source":["# sum(val_data_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1shQMpp9LH-","executionInfo":{"status":"ok","timestamp":1610020384276,"user_tz":-120,"elapsed":248181,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"e17d4226-34b7-428a-eb38-76167d0349fc"},"source":["# Generate the validation sentences\n","generated_sentences = []\n","model.to(torch.device('cuda'))\n","model.eval()\n","start_time = time.time()\n","print(\"Starting validation sentence generation\")\n","for i, batch in enumerate(val_gen_dataset):\n","  input_ids = batch[0].to(torch.device('cuda'))\n","  attn_mask = batch[1].to(torch.device('cuda'))\n","  for j in range(GEN_BATCH_SIZE):\n","    try:\n","      new_sents = model.generate_text(text = input_ids[j].unsqueeze(0), max_len = 32, \n","                                      attn_mask = attn_mask[j].unsqueeze(0), is_batch=True)\n","      generated_sentences.append(new_sents)\n","    except: \n","      continue\n","\n","  if i % 4 == 0 and i != 0:\n","    print(\"Currently on batch {} of {} - {:.2%} completed\".format(i, len(val_gen_dataset), i / len(val_gen_dataset)))\n","    time_per_batch = str(datetime.timedelta(seconds = int(round((time.time() - start_time)))))\n","    print(\"Time per batch: {}s\".format(time_per_batch))\n","    start_time = time.time()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting validation sentence generation\n","Currently on batch 4 of 32 - 12.50% completed\n","Time per batch: 0:00:40s\n","Currently on batch 8 of 32 - 25.00% completed\n","Time per batch: 0:00:32s\n","Currently on batch 12 of 32 - 37.50% completed\n","Time per batch: 0:00:32s\n","Currently on batch 16 of 32 - 50.00% completed\n","Time per batch: 0:00:30s\n","Currently on batch 20 of 32 - 62.50% completed\n","Time per batch: 0:00:31s\n","Currently on batch 24 of 32 - 75.00% completed\n","Time per batch: 0:00:32s\n","Currently on batch 28 of 32 - 87.50% completed\n","Time per batch: 0:00:32s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NLbvqihEAGpZ"},"source":["# Do some minor cleaning, replacing double spaces and deleting the extra space at the end of the sentences\n","gen_sent = [re.sub(r' {2,4}|(?<=[.!])( )', '', sent[0]) for sent in generated_sentences]\n","gen_sent = [re.sub(r\"^ \", \"\", sent) for sent in gen_sent]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U247O8f1fowL","executionInfo":{"status":"ok","timestamp":1610020384283,"user_tz":-120,"elapsed":230420,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"d36c1571-5495-4bd1-b5b3-d9ff070dd7fb"},"source":["gen_sent[15:25]#, val_data.loc[:10, 'Context']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"don't go eat here or pick up food.\",\n"," 'they provide great service and even better humor.',\n"," \"it's like people forget it's hand washed and don't deserve it.\",\n"," 'very friendly, super clean, and extremely polite!',\n"," \"a lot of hidden gems in scottsdale's shopping center.\",\n"," \"service was good but the food wasn't great.\",\n"," 'very unprofessional rude!',\n"," 'there is no where inside for people to sit while waiting for a table.',\n"," \"i didn't even care to try this dish.\",\n"," 'wow what a rip off!']"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"5eQx-aI7nP_t"},"source":["bert_tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","# Load the pre-trained BERT classifier\n","bert_model = torch.load(base_dir + '/checkpoint_files/yelp_classifier_1epoch.pth')\n","model.to(torch.device('cuda'))\n","bert_model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV4ia-3EL-gW","executionInfo":{"status":"ok","timestamp":1610020386590,"user_tz":-120,"elapsed":2293,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"f3ae6998-7b7f-44a2-c179-cc66d03c43d7"},"source":["test_gen_ids = []\n","test_gen_masks = []\n","\n","\n","print(\"Starting Validation Set Tokenization: Generated tokens\")\n","\n","for sent in gen_sent:\n","  test_enc = bert_tokenizer(sent, max_length=32, truncation = True,\n","                               pad_to_max_length = True, return_tensors = 'pt',\n","                               return_attention_mask = True)\n","  test_gen_ids.append(test_enc['input_ids'])\n","  test_gen_masks.append(test_enc['attention_mask'])\n","\n","test_gen_ids = torch.cat(test_gen_ids, dim = 0)\n","test_gen_masks = torch.cat(test_gen_masks, dim = 0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting Validation Set Tokenization: Generated tokens\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tijrQYeGeKzj"},"source":["val_data_y =  val_data['Original'].str.contains('<pos>') * 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HhG8-FNaKit"},"source":["# val_data_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fh6qGsQcW0sQ"},"source":["# Reverse the labels because we're testing translations\n","translation_val_y = 1 - val_data_y\n","\n","# Create the DataLoader object for validation\n","if do_translation:\n","  test_gen_dataset = TensorDataset(test_gen_ids, test_gen_masks, torch.tensor(np.array(translation_val_y)))\n","else:\n","  test_gen_dataset = TensorDataset(test_gen_ids, test_gen_masks, torch.tensor(np.array(val_data_y)))\n","test_gen_dataset = DataLoader(test_gen_dataset, batch_size = GEN_BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AC7puK8WzLr","executionInfo":{"status":"ok","timestamp":1610020390489,"user_tz":-120,"elapsed":6154,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"50871b6f-0700-4703-8d26-41cfde3fdf5d"},"source":["# Get the validation accuracy of the data\n","device = torch.device('cuda')\n","preds = []\n","start_time = time.time()\n","for i, batch in enumerate(test_gen_dataset):\n","  if i % 16 == 0:\n","    print(\"Currently conducting validation on batch {} of {}\".format(i, len(test_gen_dataset)))\n","    print(\"Time per 16 batches: {}\".format(str(datetime.timedelta(seconds = int(round((time.time() - start_time)))))))\n","    start_time = time.time()\n","\n","  # Move the tokens to the device\n","  input_ids = batch[0].to(device)\n","  attention_masks = batch[1].to(device)\n","  labels = batch[2].to(device)\n","\n","  model_output = bert_model(input_ids = input_ids, \n","                        attention_mask = attention_masks,\n","                        labels = labels)\n","  \n","  # total_val_loss += loss.item()\n","  loss = model_output[0]\n","  logits = model_output[1]\n","  # Get the predictions\n","  preds.append(logits.cpu().detach().numpy().argmax(-1))\n","\n","# Get the accuracy of the predictions\n","val_predictions = []\n","for it in preds:\n","  val_predictions.extend(it)\n","\n","if do_translation:\n","  val_acc = np.mean(val_predictions == translation_val_y)\n","else:\n","  val_acc = np.mean(val_predictions == val_data_y)\n","\n","print(\"Validation Accuracy {:.2f}%\".format(val_acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Currently conducting validation on batch 0 of 32\n","Time per 16 batches: 0:00:00\n","Currently conducting validation on batch 16 of 32\n","Time per 16 batches: 0:00:02\n","Validation Accuracy 50.60%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LRdTmuuMXojk"},"source":["# val_predictions == val_data_y\n","# val_data[val_predictions != val_data_y]\n","\n","test_data_df = val_data.copy()\n","test_data_df['Generated'] = gen_sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thFc36T5fV7x","executionInfo":{"status":"ok","timestamp":1610020390499,"user_tz":-120,"elapsed":6139,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"b3c33975-bd70-4026-ee89-7827f6007ac9"},"source":["test_data_df.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Original', 'Masked', 'Label', 'Generated'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"jgYVoUYla-QL"},"source":["test_data_df.rename(columns = {'Labels':'Actual_Label'}, inplace = True)\n","test_data_df['Predicted_Label'] = val_predictions\n","# test_data_df.drop(columns = ['Masked_Words', 'Masked_Indices', 'Context'], axis = 1, inplace = True)\n","# test_data_df.drop(columns = ['Masked_Words', 'Masked_Indices'], axis = 1, inplace = True)\n","# test_data_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWar_MJcZt5b","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1610020390503,"user_tz":-120,"elapsed":6120,"user":{"displayName":"Neil Sinclair","photoUrl":"","userId":"02050606542070245481"}},"outputId":"44fea9f4-294c-4385-9c1f-e15aca0aee8f"},"source":["# Fix up some weird issue with the predictions\n","test_data_df['Actual_Label'] = test_data_df['Original'].str.contains('<pos>') * 1\n","test_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Original</th>\n","      <th>Masked</th>\n","      <th>Label</th>\n","      <th>Generated</th>\n","      <th>Predicted_Label</th>\n","      <th>Actual_Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31573</th>\n","      <td>&lt;pos&gt; everyone there is super friendly.</td>\n","      <td>&lt;pos&gt; everyone there is &lt;mask&gt; &lt;mask&gt;.</td>\n","      <td>1</td>\n","      <td>everyone there is very unprofessional.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43033</th>\n","      <td>&lt;pos&gt; it's the perfect neighborhood bar.</td>\n","      <td>&lt;pos&gt; it &lt;mask&gt; the &lt;mask&gt; neighborhood bar.</td>\n","      <td>1</td>\n","      <td>it hits the spot neighborhood bar.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2775</th>\n","      <td>&lt;neg&gt; i can laugh about it now, i was pissed a...</td>\n","      <td>&lt;neg&gt; i can &lt;mask&gt; about it now, i was &lt;mask&gt; ...</td>\n","      <td>0</td>\n","      <td>i can dream about it now, i was drunk at the t...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16895</th>\n","      <td>&lt;neg&gt; she's just not very bright.</td>\n","      <td>&lt;neg&gt; she's just &lt;mask&gt; &lt;mask&gt; &lt;mask&gt;.</td>\n","      <td>0</td>\n","      <td>she's just a sweetheart.</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6954</th>\n","      <td>&lt;neg&gt; we never changed our mind and i'm _num_ ...</td>\n","      <td>&lt;neg&gt; we &lt;mask&gt; changed our mind and i'm _num_...</td>\n","      <td>0</td>\n","      <td>we just changed our mind and i'm _num_ minutes...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Original  ... Actual_Label\n","31573            <pos> everyone there is super friendly.  ...            1\n","43033           <pos> it's the perfect neighborhood bar.  ...            1\n","2775   <neg> i can laugh about it now, i was pissed a...  ...            0\n","16895                  <neg> she's just not very bright.  ...            0\n","6954   <neg> we never changed our mind and i'm _num_ ...  ...            0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"hb0oyBcrcpkr"},"source":["# Filters for \"correct\" translations from positive to negative\r\n","test_data_df_correct = test_data_df[test_data_df['Actual_Label'] != test_data_df['Predicted_Label']]\r\n","\r\n","# Filters for \"incorrect\" translations from one sentiment to the other\r\n","test_data_df_incorrect = test_data_df[test_data_df['Actual_Label'] == test_data_df['Predicted_Label']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azYNlNwjbD0V"},"source":["test_data_df_incorrect.to_csv(base_dir + 'translated_results_incorrect_07012021.csv', index = False)\r\n","test_data_df_correct.to_csv(base_dir + 'translated_results_correct_07012021.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBZ55lL8j8mK"},"source":[""],"execution_count":null,"outputs":[]}]}